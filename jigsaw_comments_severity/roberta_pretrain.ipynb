{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "n_seeds = 1\n",
    "n_splits = 10\n",
    "n_epochs = 1\n",
    "\n",
    "RESULT_TXT = f\"roberta-{datetime.now().strftime('%Y%m%d-%H%M%S')}.txt\"\n",
    "RESULT_PATH = \"../models/fine-tune-roberta\"\n",
    "\n",
    "OUT_DROPOUT = 0.3\n",
    "\n",
    "TRAIN_ON_N_SPLITS = 1\n",
    "\n",
    "SUBGROUP_NEGATIVE_WEIGHT_COEF = 1\n",
    "\n",
    "BERT_N_LAST_LAYER = 4\n",
    "BERT_HIDDEN_SIZE = 768\n",
    "\n",
    "BERT_MODEL_PATH = 'roberta-base'\n",
    "BERT_DO_LOWER = False\n",
    "\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-20220203-234253.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT_TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.mkdir(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.9G\n",
    "\n",
    "train = pd.read_csv('../../data/all_data.csv').rename(columns={'toxicity': 'target'})\n",
    "\n",
    "train = train[train['comment_text'].str.len() > 1].reset_index(drop=True)\n",
    "\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&',\n",
    "#           '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "#           '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›', \n",
    "#           '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "#  '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', \n",
    "#           '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "#  '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯',\n",
    "#           '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "#  '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔',\n",
    "#           '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '\\n', '\\r']\n",
    "\n",
    "# with open('../../jigsaw-crawl-300d-2M.joblib', 'rb') as f:\n",
    "#     crawl_emb_dict = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../google-profanity-words/profanity.js', 'r') as handle:\n",
    "#     p_words = handle.readlines()\n",
    "    \n",
    "# set_puncts = set(puncts)\n",
    "\n",
    "# p_word_set = set([t.replace('\\n', '') for t in p_words])\n",
    "\n",
    "# def sentence_fetures(text):\n",
    "#     word_list = text.split()\n",
    "#     word_count = len(word_list)\n",
    "#     n_upper = len([word for word in word_list if any([c.isupper() for c in word])])\n",
    "#     n_unique = len(set(word_list))\n",
    "#     n_ex = word_list.count('!')\n",
    "#     n_que = word_list.count('?')\n",
    "#     n_puncts = len([word for word in word_list if word in set_puncts])\n",
    "#     n_prof = len([word for word in word_list if word in p_word_set])\n",
    "#     n_oov = len([word for word in word_list if word not in crawl_emb_dict])\n",
    "    \n",
    "#     return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov\n",
    "\n",
    "# sentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# feature_dict = defaultdict(list)\n",
    "\n",
    "# for text in tqdm(train['comment_text']):\n",
    "#     feature_list = sentence_fetures(text)\n",
    "#     for i_feature, feature_name in enumerate(sentence_feature_cols):\n",
    "#         feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])\n",
    "        \n",
    "# sentence_df = pd.DataFrame.from_dict(feature_dict)\n",
    "\n",
    "# for col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:\n",
    "#     sentence_df[col + '_ratio'] = sentence_df[col] / sentence_df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_df = pd.read_csv('../input/sentencefeaturesoov/sentence_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_df.to_parquet('sent_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.read_parquet('sent_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>n_ex</th>\n",
       "      <th>n_que</th>\n",
       "      <th>n_puncts</th>\n",
       "      <th>n_prof</th>\n",
       "      <th>n_oov</th>\n",
       "      <th>n_upper_ratio</th>\n",
       "      <th>n_unique_ratio</th>\n",
       "      <th>n_ex_ratio</th>\n",
       "      <th>n_que_ratio</th>\n",
       "      <th>n_puncts_ratio</th>\n",
       "      <th>n_prof_ratio</th>\n",
       "      <th>n_oov_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  n_upper  n_unique  n_ex  n_que  n_puncts  n_prof  n_oov  \\\n",
       "0          36        2        33     0      0         0       0      3   \n",
       "1          11        2        11     0      0         0       0      0   \n",
       "2          18        3        18     0      0         0       0      0   \n",
       "3          24        2        22     0      0         0       0      4   \n",
       "4           9        2         9     0      0         0       1      1   \n",
       "\n",
       "   n_upper_ratio  n_unique_ratio  n_ex_ratio  n_que_ratio  n_puncts_ratio  \\\n",
       "0       0.055556        0.916667         0.0          0.0             0.0   \n",
       "1       0.181818        1.000000         0.0          0.0             0.0   \n",
       "2       0.166667        1.000000         0.0          0.0             0.0   \n",
       "3       0.083333        0.916667         0.0          0.0             0.0   \n",
       "4       0.222222        1.000000         0.0          0.0             0.0   \n",
       "\n",
       "   n_prof_ratio  n_oov_ratio  \n",
       "0      0.000000     0.083333  \n",
       "1      0.000000     0.000000  \n",
       "2      0.000000     0.000000  \n",
       "3      0.000000     0.166667  \n",
       "4      0.111111     0.111111  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_feature_mat = sentence_df.values\n",
    "del sentence_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXICITY_COLUMN = 'target'\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', \n",
    "    'christian', 'jewish', 'muslim', 'black', \n",
    "    'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "\n",
    "subgroup_bool_train = (train[identity_columns].fillna(0) >= 0.5)\n",
    "toxic_bool_train = (train[TOXICITY_COLUMN].fillna(0) >= 0.5)\n",
    "subgroup_negative_mask = (\n",
    "    subgroup_bool_train.values.sum(axis=1).astype(bool) & (~toxic_bool_train)\n",
    ")\n",
    "\n",
    "sample_weight = np.ones((y_train.shape[0],))\n",
    "sample_weight += SUBGROUP_NEGATIVE_WEIGHT_COEF * subgroup_negative_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_target = np.hstack([\n",
    "    np.expand_dims(y_train, axis=1), \n",
    "    y_aux_train,\n",
    "    np.expand_dims(sample_weight, axis=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row, max_len, tokenizer):\n",
    "    tokenized = tokenizer.encode_plus(\n",
    "        row,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    return tokenized['input_ids'], tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_x_tokenized = train['comment_text'].parallel_apply(\n",
    "#     lambda x: tokenize(x, MAX_LEN, tokenizer)\n",
    "# )\n",
    "# x_train_indexed = df_x_tokenized\n",
    "# pd.DataFrame(x_train_indexed).to_parquet('x_train_tokenized_roberta.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_indexed = pd.read_parquet('x_train_tokenized_roberta.parquet')['comment_text']\n",
    "x_train_indexed = x_train_indexed.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, tokenized, sent_features, labels):\n",
    "        self.labels = labels\n",
    "        self.sent_features = sent_features\n",
    "        self.tokenized = tokenized\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"ids\": torch.tensor(self.tokenized[idx][0], dtype=torch.long),\n",
    "            \"attention\": torch.tensor(self.tokenized[idx][1], dtype=torch.long),\n",
    "            \"sent_features\": torch.tensor(self.sent_features[idx], dtype=torch.float),\n",
    "            \"labels\": torch.tensor(self.labels[idx][:-1], dtype=torch.float),\n",
    "            \"sample_weight\": torch.tensor(self.labels[idx][-1], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_aux_targets, num_sentence_features):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.bert_model = RobertaModel.from_pretrained(BERT_MODEL_PATH)\n",
    "        self.dropout = nn.Dropout(OUT_DROPOUT)\n",
    "        \n",
    "        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n",
    "        \n",
    "        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n",
    "        self.linear1 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        self.linear_out = nn.Linear(n_hidden, 1)\n",
    "        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n",
    "        \n",
    "    def forward(self, ids, attention, sentence_features):\n",
    "        \n",
    "        bert_output = self.bert_model(ids, attention)[1]\n",
    "        \n",
    "        bert_output = self.dropout(bert_output)\n",
    "        \n",
    "        h_sentence = self.linear_sentence1(sentence_features)\n",
    "        \n",
    "        h_cat = torch.cat((bert_output, h_sentence), 1)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear1(h_cat))\n",
    "        \n",
    "        hidden = h_cat + h_conc_linear1\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.old_time = time.time()\n",
    "        print('start mearsure elapsed times')\n",
    "        \n",
    "    def stamp(self, comment):\n",
    "        print(comment + f': from start {time.time() - self.start_time: .1f}, from old {self.old_time - - self.start_time: .1f}')\n",
    "        self.old_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, i_batch, min_lr, max_lr, n_batch_all, warm_up_batch_ratio):\n",
    "    n_batch_warmed = int(n_batch_all * warm_up_batch_ratio)\n",
    "    if i_batch > n_batch_warmed:\n",
    "        optimizer.param_groups[0]['lr'] = max_lr\n",
    "    else:\n",
    "        optimizer.param_groups[0]['lr'] = (max_lr - min_lr) / n_batch_warmed * i_batch + min_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start mearsure elapsed times\n",
      "start seed 0\n",
      "seed start: from start  0.0, from old  3287841997.7\n",
      "epoch start: from start  0.2, from old  3287841997.7\n",
      "start fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 502366208\n",
      "i_epoch 0 start: from start  5.0, from old  3287841997.9\n",
      "epoch_start: 0 502366208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56232' class='' max='56232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [56232/56232 3:20:11<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.84466552734375\n",
      "163.30393709994777\n",
      "126.8285970165955\n",
      "101.47682644916927\n",
      "87.11897114684754\n",
      "78.44788923282586\n",
      "72.27723314599467\n",
      "67.46863379866183\n",
      "63.60877637113078\n",
      "60.36107162153814\n",
      "57.80496781165307\n",
      "55.553485984698305\n",
      "53.63188196717452\n",
      "52.00656595750555\n",
      "50.498793997482096\n",
      "49.26874797650768\n",
      "48.0881362220483\n",
      "47.04699115315863\n",
      "46.048747523369755\n",
      "45.19742174665279\n",
      "44.46665450109952\n",
      "43.75192298280915\n",
      "43.06552329663524\n",
      "42.356621949478935\n",
      "41.77575748813793\n",
      "41.24536102040202\n",
      "40.75728449221988\n",
      "40.32974110193405\n",
      "39.91799664965531\n",
      "39.51310605618182\n",
      "39.13294725503893\n",
      "38.78931291008795\n",
      "38.425390859053906\n",
      "38.10369944984138\n",
      "37.80538531788235\n",
      "37.5160671496725\n",
      "37.248755591407615\n",
      "36.98626078930587\n",
      "36.7164415245588\n",
      "36.43253567914906\n",
      "36.24234705381291\n",
      "36.02393453349662\n",
      "35.84743369735158\n",
      "35.62750192443539\n",
      "35.41937977117561\n",
      "35.266895877390645\n",
      "35.095686321076144\n",
      "34.95444351053065\n",
      "34.794855106177366\n",
      "34.63792223629721\n",
      "34.46639003049991\n",
      "34.33150405777223\n",
      "34.199866066848145\n",
      "34.06108063136693\n",
      "33.92017096825119\n",
      "33.787861547433685\n",
      "33.668862065892625\n",
      "33.56316868854476\n",
      "33.45982619101293\n",
      "33.35204305335276\n",
      "33.246738252590504\n",
      "33.15650284663514\n",
      "33.049372155903576\n",
      "32.92635064283225\n",
      "32.84123145600927\n",
      "32.72901260161506\n",
      "32.632935707830256\n",
      "32.535205881553985\n",
      "32.43809160781667\n",
      "32.358366603143764\n",
      "32.29778462160555\n",
      "32.21285966617459\n",
      "32.134531267913346\n",
      "32.072907570404084\n",
      "31.995727567604433\n",
      "31.91685407998609\n",
      "31.845106183408262\n",
      "31.78653904339382\n",
      "31.732106938329725\n",
      "31.64443475876318\n",
      "31.578941975335987\n",
      "31.50614198521764\n",
      "31.440997600206558\n",
      "31.40291545293716\n",
      "31.34161486436662\n",
      "31.28753149401453\n",
      "31.221802708326752\n",
      "31.175026905617486\n",
      "31.133182866263695\n",
      "31.070272072448233\n",
      "31.01959324343948\n",
      "30.97676140878793\n",
      "30.929664494952174\n",
      "30.89976583600185\n",
      "30.86694340568415\n",
      "30.816226004701\n",
      "30.76677479682869\n",
      "30.723656405101007\n",
      "30.674050032830994\n",
      "30.632996889861253\n",
      "30.594482539498966\n",
      "30.549084028410753\n",
      "30.503852413771327\n",
      "30.474129778796545\n",
      "30.426724986197478\n",
      "30.381124458271213\n",
      "30.343920647114047\n",
      "30.318608295270277\n",
      "30.28589337000791\n",
      "30.246802860745028\n",
      "30.209123373876842\n",
      "30.174307614809198\n",
      "30.13533657872776\n",
      "30.095419592207993\n",
      "30.057119357168713\n",
      "30.020033995697432\n",
      "29.98457189847987\n",
      "29.946210256734165\n",
      "29.897077792317734\n",
      "29.867519857342348\n",
      "29.83472104335207\n",
      "29.80185695124425\n",
      "29.768738406435915\n",
      "29.74472905961336\n",
      "29.72152946987572\n",
      "29.682561663034182\n",
      "29.644303652040826\n",
      "29.61500158318196\n",
      "29.585364899070232\n",
      "29.56187371554906\n",
      "29.536234710631156\n",
      "29.506664028186652\n",
      "29.4888910328689\n",
      "29.45436114173627\n",
      "29.428665263324913\n",
      "29.40124293958052\n",
      "29.384927577704563\n",
      "29.355747361068666\n",
      "29.329347192960395\n",
      "29.313202646394213\n",
      "29.29373922137548\n",
      "29.27976532616807\n",
      "29.249944332376113\n",
      "29.224918945512023\n",
      "29.19901957265884\n",
      "29.18049070159531\n",
      "29.15168380626268\n",
      "29.12993756071028\n",
      "29.10008305010381\n",
      "29.08029311388565\n",
      "29.05698257290151\n",
      "29.044905373800844\n",
      "29.02896786206616\n",
      "29.007412052272965\n",
      "28.99269812996144\n",
      "28.979736363500958\n",
      "28.959642219372295\n",
      "28.941746790571113\n",
      "28.92008931373812\n",
      "28.903798523102758\n",
      "28.878555993155892\n",
      "28.862377535874085\n",
      "28.848984487512496\n",
      "28.836258640831055\n",
      "28.81314419650514\n",
      "28.796166182214524\n",
      "28.778350366999476\n",
      "28.76170923855339\n",
      "28.737256552405203\n",
      "28.7187058326514\n",
      "28.707908604847784\n",
      "28.689444944430676\n",
      "28.675376150013577\n",
      "28.6626545237872\n",
      "28.644076527908517\n",
      "28.626591689436378\n",
      "28.60766063634052\n",
      "28.586086118243507\n",
      "28.57037928682868\n",
      "28.55408733220455\n",
      "28.53755921868773\n",
      "28.52313848134793\n",
      "28.510551713520428\n",
      "28.49511609693139\n",
      "28.487823173343006\n",
      "28.474087821722712\n",
      "28.46231154829999\n",
      "28.44569638145014\n",
      "28.432146426978424\n",
      "28.42111395631579\n",
      "28.406371069182835\n",
      "28.388851904381035\n",
      "28.370266732260134\n",
      "28.356646209175455\n",
      "28.346247909745077\n",
      "28.3333784398235\n",
      "28.321560509976887\n",
      "28.309335389734372\n",
      "28.294299577560142\n",
      "28.285215325693613\n",
      "28.27257224501541\n",
      "28.26043694669658\n",
      "28.249553720713028\n",
      "28.235328240838758\n",
      "28.225774031259004\n",
      "28.211392730766665\n",
      "28.19775810186611\n",
      "28.184385648217617\n",
      "28.16981184857951\n",
      "28.171332321328517\n",
      "28.160180788347592\n",
      "28.145080502974096\n",
      "28.134692941059015\n",
      "28.131177518105968\n",
      "28.120035066542673\n",
      "28.108105845176578\n",
      "28.099542709253633\n",
      "28.088802380867474\n",
      "28.077968652770803\n",
      "28.067959358004053\n",
      "28.055237347643722\n",
      "28.04401015473741\n",
      "28.034086948074492\n",
      "28.021639527026263\n",
      "28.01524616687363\n",
      "28.002876659948704\n",
      "27.98960462974598\n",
      "27.97419820380712\n",
      "27.969297193298267\n",
      "27.962431364437155\n",
      "27.956599449086234\n",
      "27.955125498908934\n",
      "27.943325311899915\n",
      "27.937069206574225\n",
      "27.91980335210247\n",
      "27.905561680093957\n",
      "27.898211258133863\n",
      "27.885917673512935\n",
      "27.87279263795271\n",
      "27.86354579536901\n",
      "27.857532338427333\n",
      "27.844388136245033\n",
      "27.835131361712985\n",
      "27.824339590908142\n",
      "27.819335018688086\n",
      "27.81115870399673\n",
      "27.808063821818575\n",
      "27.794221392160853\n",
      "27.78055234672725\n",
      "27.773881095934332\n",
      "27.76076292622581\n",
      "27.75109412260661\n",
      "27.741322381388294\n",
      "27.733406902099286\n",
      "27.72604573962679\n",
      "27.719964022169133\n",
      "27.706049255791886\n",
      "27.69546364704499\n",
      "27.686590368168044\n",
      "27.67605313422261\n",
      "27.66415244479128\n",
      "27.654769635448044\n",
      "27.647780798067643\n",
      "27.635134124813874\n",
      "27.631200771549246\n",
      "27.626744479009563\n",
      "27.618799770377166\n",
      "27.60862002421495\n",
      "27.602409235225995\n",
      "27.589767403622275\n",
      "27.58290449009441\n",
      "27.577548760003495\n",
      "27.56848782186346\n",
      "27.557815015833437\n",
      "27.551078278428207\n",
      "27.544652056882157\n",
      "27.53996141611383\n",
      "27.532121401944103\n",
      "27.523574655422447\n",
      "27.51765560043366\n",
      "27.50972100863401\n",
      "27.50498768148174\n",
      "27.496017413952472\n",
      "27.485304912726058\n",
      "27.47671777581435\n",
      "27.470615875929692\n",
      "27.466744205842573\n",
      "27.458931948066127\n",
      "27.457526408791157\n",
      "27.450425997097508\n",
      "27.44700464275491\n",
      "27.43839003816019\n",
      "27.43327163866965\n",
      "27.426379034044896\n",
      "27.421388856805365\n",
      "27.415663868608096\n",
      "27.409931049206456\n",
      "27.404826492321\n",
      "27.397185901120096\n",
      "27.39518806042191\n",
      "27.389570485663047\n",
      "27.38661683597009\n",
      "27.378325374413553\n",
      "27.371989511349057\n",
      "27.362838343363727\n",
      "27.353507312515642\n",
      "27.343092170425\n",
      "27.335279586163644\n",
      "27.32779425169941\n",
      "27.323470675209602\n",
      "27.314964307492357\n",
      "27.310261958550228\n",
      "27.305404826412712\n",
      "27.30193711078181\n",
      "27.299247510227968\n",
      "27.295448523968545\n",
      "27.291923917157337\n",
      "27.286029646018413\n",
      "27.279335846155266\n",
      "27.278593182814152\n",
      "27.272448404018352\n",
      "27.266425770237632\n",
      "27.2602752054009\n",
      "27.256341686374554\n",
      "27.249227273766262\n",
      "27.244407617261984\n",
      "27.238890335861953\n",
      "27.233698985258822\n",
      "27.229960570375074\n",
      "27.226204870725518\n",
      "27.22292879837823\n",
      "27.22089450141317\n",
      "27.216671802815117\n",
      "27.21290911334022\n",
      "27.20596274695444\n",
      "27.198156720663\n",
      "27.193299215821465\n",
      "27.190041149289723\n",
      "27.187762191410528\n",
      "27.18684762438674\n",
      "27.182703175596206\n",
      "27.179310861373136\n",
      "27.1745002908925\n",
      "27.17398195199162\n",
      "27.170252401701728\n",
      "27.16521437280555\n",
      "27.158958849371253\n",
      "27.156506514261373\n",
      "27.15026252087031\n",
      "27.14988818192823\n",
      "27.14251294572954\n",
      "27.13980379683595\n",
      "27.13385935713732\n",
      "27.12832921547024\n",
      "27.123087905355135\n",
      "27.119689079228362\n",
      "27.114424926344423\n",
      "27.11499787482864\n",
      "27.114514981763683\n",
      "27.109678238229343\n",
      "27.102020806085566\n",
      "27.09969747190987\n",
      "27.096368483925936\n",
      "27.089293371681695\n",
      "27.08567833556588\n",
      "27.080898374607294\n",
      "27.08030815689405\n",
      "27.07554486340903\n",
      "27.070679643076986\n",
      "27.07074615194954\n",
      "27.06108624114046\n",
      "27.05678502313114\n",
      "27.051571760615726\n",
      "27.043748460504116\n",
      "27.03753366819923\n",
      "27.034502719581003\n",
      "27.029998814196368\n",
      "27.027938627329704\n",
      "27.020859153500115\n",
      "27.015377816278033\n",
      "27.01264939604676\n",
      "27.00986204344827\n",
      "27.00345543930566\n",
      "27.00063264350356\n",
      "26.997095114216048\n",
      "26.99103366385831\n",
      "26.98378303974318\n",
      "26.98030951731501\n",
      "26.976121203106292\n",
      "26.971814860887218\n",
      "26.96673866389443\n",
      "26.962627953749625\n",
      "26.956333529976252\n",
      "26.94825282828116\n",
      "26.944966430818486\n",
      "26.94112788153034\n",
      "26.938206864108924\n",
      "26.929223970623298\n",
      "26.92435090377287\n",
      "26.918759208732833\n",
      "26.91387086621386\n",
      "26.907252261622055\n",
      "26.90455816971159\n",
      "26.90240908266578\n",
      "26.896948357253486\n",
      "26.896304081623672\n",
      "26.889914799974438\n",
      "26.884060936019107\n",
      "26.877373992244138\n",
      "26.8742579224436\n",
      "26.869997443119168\n",
      "26.86479040866426\n",
      "26.861709490635075\n",
      "26.85752558364715\n",
      "26.852455892567704\n",
      "26.85119903298166\n",
      "26.84530232347596\n",
      "26.840937053090116\n",
      "26.83651159952732\n",
      "26.83190151524229\n",
      "26.8268936252183\n",
      "26.825986791757366\n",
      "26.82256789451634\n",
      "26.818433034724652\n",
      "26.81649227942592\n",
      "26.812203071063603\n",
      "26.80690954873289\n",
      "26.802936960295693\n",
      "26.798190553935704\n",
      "26.798178851161257\n",
      "26.797048677597175\n",
      "26.794029323084636\n",
      "26.79141987968613\n",
      "26.786961885966626\n",
      "26.782030762149695\n",
      "26.779486139364174\n",
      "26.77554431719172\n",
      "26.77296759874329\n",
      "26.769579172139842\n",
      "26.7667197563016\n",
      "26.763546534450036\n",
      "26.75982436157214\n",
      "26.759085251700782\n",
      "26.755909797292336\n",
      "26.75175738180546\n",
      "26.748821715741116\n",
      "26.743501985803565\n",
      "26.74199500800443\n",
      "26.741641306168816\n",
      "26.736548961444765\n",
      "26.73375426270994\n",
      "26.731171006831367\n",
      "26.72539614797712\n",
      "26.721261146092203\n",
      "26.71630172310851\n",
      "26.7132227995776\n",
      "26.707049436578206\n",
      "26.70527373736245\n",
      "26.702863580870687\n",
      "26.70139933444818\n",
      "26.698061039602184\n",
      "26.693357570122586\n",
      "26.690764458736133\n",
      "26.688660147627736\n",
      "26.686097255028212\n",
      "26.682060519397055\n",
      "26.676913300859233\n",
      "26.675555899695553\n",
      "26.67375419148064\n",
      "26.668094616272192\n",
      "26.664392273653462\n",
      "26.659147209676824\n",
      "26.658607464534132\n",
      "26.654343352343314\n",
      "26.65287100048785\n",
      "26.648786240744716\n",
      "26.645553030308267\n",
      "26.642544134070878\n",
      "26.63891538440756\n",
      "26.636171775233414\n",
      "26.6334705798716\n",
      "26.6286870385279\n",
      "26.626292114685572\n",
      "26.62566869132171\n",
      "26.623410316033883\n",
      "26.62118238676322\n",
      "26.6201500362831\n",
      "26.61655899137029\n",
      "26.614140580646847\n",
      "26.613111310008993\n",
      "26.61331528320728\n",
      "26.610656238156267\n",
      "26.607759190997427\n",
      "26.60646077551244\n",
      "26.60585558755912\n",
      "26.603795260059115\n",
      "26.6019092238898\n",
      "26.599839897727186\n",
      "26.597016355129195\n",
      "26.594605973401325\n",
      "26.591904947761105\n",
      "26.58934102849602\n",
      "26.587630546410477\n",
      "26.58654307142664\n",
      "26.58535203777422\n",
      "26.584972436422262\n",
      "26.581129043536357\n",
      "26.57711892879347\n",
      "26.57719564867762\n",
      "26.575515703893508\n",
      "26.574313917360303\n",
      "26.571233900229604\n",
      "26.56891430692024\n",
      "26.56676501888148\n",
      "26.563218814459635\n",
      "26.560873523860003\n",
      "26.55986831415573\n",
      "26.558372716263058\n",
      "26.55591122158402\n",
      "26.55371002130124\n",
      "26.551469406786612\n",
      "26.548626336357476\n",
      "26.544386493509084\n",
      "26.539490201441737\n",
      "26.536571497171085\n",
      "26.532279868917996\n",
      "26.530173605706416\n",
      "26.52835887021438\n",
      "26.525012370142594\n",
      "26.52132916857472\n",
      "26.517417010018352\n",
      "26.515837249009458\n",
      "26.51178206579324\n",
      "26.507931917358416\n",
      "26.506845044597714\n",
      "26.5049359273341\n",
      "26.501318061759438\n",
      "26.49997266466015\n",
      "26.498666904377707\n",
      "26.498918886664704\n",
      "26.497389534981533\n",
      "26.494734289007102\n",
      "26.491693587679787\n",
      "26.48890473670901\n",
      "26.488367778374315\n",
      "26.48506440069235\n",
      "26.48203927270113\n",
      "26.47954551696045\n",
      "26.47688422523531\n",
      "26.477199348729656\n",
      "26.47337332573963\n",
      "26.47081848074504\n",
      "26.47010243565049\n",
      "26.466843369843456\n",
      "26.464026346699157\n",
      "26.464560846279696\n",
      "26.461355603651803\n",
      "26.458112300121794\n",
      "26.45553952888234\n",
      "26.452983132965592\n",
      "26.45241524918978\n",
      "26.44879311708305\n",
      "26.445028044698468\n",
      "after dev all batch: from start  12016.7, from old  3287842002.7\n",
      "epoch start: from start  12017.3, from old  3287854014.5\n",
      "start fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2516131328\n",
      "i_epoch 0 start: from start  12020.2, from old  3287854015.1\n",
      "epoch_start: 0 1510761984\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56232' class='' max='56232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [56232/56232 3:19:47<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.58250427246094\n",
      "147.79249799369586\n",
      "111.80507523266238\n",
      "91.64158809066215\n",
      "79.86168566011729\n",
      "72.44980693291761\n",
      "67.6655652050964\n",
      "63.73287893940141\n",
      "60.44011179844241\n",
      "57.33376490712563\n",
      "54.89059815040002\n",
      "52.86539552083998\n",
      "51.179980125554295\n",
      "49.63631510313065\n",
      "48.29144381932239\n",
      "47.15569990400788\n",
      "46.04413257696567\n",
      "45.09930628149177\n",
      "44.2018255581133\n",
      "43.37381924233394\n",
      "42.65627641835134\n",
      "42.01604485500432\n",
      "41.4227203792033\n",
      "40.870921666701946\n",
      "40.35713836283845\n",
      "39.90331878646857\n",
      "39.452887281002056\n",
      "39.049944362477966\n",
      "38.68893464009109\n",
      "38.283899219148694\n",
      "37.91657952172325\n",
      "37.6369061276283\n",
      "37.339967135227084\n",
      "37.038262131357875\n",
      "36.78340983327716\n",
      "36.529914940265\n",
      "36.25852714224479\n",
      "36.00299051226039\n",
      "35.81991460466975\n",
      "35.58542613535776\n",
      "35.396642493176714\n",
      "35.22660502385175\n",
      "35.02243122842249\n",
      "34.831324227990955\n",
      "34.63548297823572\n",
      "34.45506798090126\n",
      "34.33366821973486\n",
      "34.15716114675104\n",
      "34.03672423806694\n",
      "33.88986571324988\n",
      "33.742539997650034\n",
      "33.60841492619428\n",
      "33.4922710669726\n",
      "33.378156485590836\n",
      "33.25210447555073\n",
      "33.128682404469416\n",
      "33.029949860878446\n",
      "32.91887796403985\n",
      "32.799080606946205\n",
      "32.72182816971037\n",
      "32.61295644697835\n",
      "32.51065916179575\n",
      "32.403756423104944\n",
      "32.33688789934341\n",
      "32.23946765750074\n",
      "32.14439351322871\n",
      "32.046869898759816\n",
      "31.976689764073207\n",
      "31.914633921570083\n",
      "31.850277210003984\n",
      "31.7661601832008\n",
      "31.693069529859066\n",
      "31.62603846948489\n",
      "31.579085529773078\n",
      "31.517947312287905\n",
      "31.44562019543431\n",
      "31.388028399785277\n",
      "31.33318581374926\n",
      "31.27077110216199\n",
      "31.217913519112525\n",
      "31.15723431514153\n",
      "31.100509687936274\n",
      "31.05606520157037\n",
      "31.002281427814246\n",
      "30.94079550259625\n",
      "30.88907817290427\n",
      "30.84183667862281\n",
      "30.799223070788035\n",
      "30.7584523915188\n",
      "30.714334092345375\n",
      "30.667130543806596\n",
      "30.62614762805318\n",
      "30.584680892451797\n",
      "30.540247654686716\n",
      "30.49633451315104\n",
      "30.46020463938061\n",
      "30.417944808612702\n",
      "30.354912566224417\n",
      "30.31251904230047\n",
      "30.256407312426514\n",
      "30.20446891854279\n",
      "30.173384026647405\n",
      "30.1324511088994\n",
      "30.10364821632792\n",
      "30.072754689608956\n",
      "30.049439542441807\n",
      "30.017819958850467\n",
      "29.9834588906128\n",
      "29.959587396134847\n",
      "29.932550239230544\n",
      "29.89495912641517\n",
      "29.862672333631007\n",
      "29.83667943946124\n",
      "29.805508788945705\n",
      "29.77753557112016\n",
      "29.734660222337368\n",
      "29.694785136763837\n",
      "29.6701345341846\n",
      "29.635892346878897\n",
      "29.61262368502792\n",
      "29.584584170585373\n",
      "29.556606679127814\n",
      "29.52015192869618\n",
      "29.496375236421297\n",
      "29.475584406130217\n",
      "29.45844235285579\n",
      "29.452533494800853\n",
      "29.430921563425642\n",
      "29.396062031713058\n",
      "29.36411648496677\n",
      "29.338590039408086\n",
      "29.318066068391165\n",
      "29.27857962627048\n",
      "29.25683838922667\n",
      "29.23352029471066\n",
      "29.205197619840874\n",
      "29.181368200018706\n",
      "29.16275929548507\n",
      "29.138109913892258\n",
      "29.10856913131018\n",
      "29.08594299573471\n",
      "29.063899142176925\n",
      "29.03706407929783\n",
      "29.013045942522446\n",
      "28.993748077956532\n",
      "28.97918231282906\n",
      "28.955108880490506\n",
      "28.939839468416846\n",
      "28.912178667225827\n",
      "28.888202999431922\n",
      "28.86311723998241\n",
      "28.84636464509559\n",
      "28.822282012720876\n",
      "28.793625012588425\n",
      "28.777944603056717\n",
      "28.76316295581574\n",
      "28.742503106017182\n",
      "28.72430700840035\n",
      "28.70272958490654\n",
      "28.685263661106955\n",
      "28.663501280127864\n",
      "28.64182517170336\n",
      "28.632285546494167\n",
      "28.616150618988957\n",
      "28.59971752638323\n",
      "28.591207120253976\n",
      "28.571599242673006\n",
      "28.556739119924934\n",
      "28.536091479564888\n",
      "28.52168622642401\n",
      "28.50749855881249\n",
      "28.48839132036147\n",
      "28.47766458261194\n",
      "28.466971791112336\n",
      "28.457239692618494\n",
      "28.439817579451525\n",
      "28.437340013615536\n",
      "28.42176633279584\n",
      "28.4069152885284\n",
      "28.390926177498613\n",
      "28.375070431770055\n",
      "28.359012112881715\n",
      "28.346783008033658\n",
      "28.32842666207425\n",
      "28.30919858568564\n",
      "28.30196478735285\n",
      "28.29169458661321\n",
      "28.271478316432162\n",
      "28.259912162554734\n",
      "28.242591663653442\n",
      "28.227565079925878\n",
      "28.21778509263736\n",
      "28.202297474430175\n",
      "28.193071059998832\n",
      "28.18608116020917\n",
      "28.172656142108288\n",
      "28.159390011780555\n",
      "28.143171476752652\n",
      "28.125925773864118\n",
      "28.117662801052973\n",
      "28.106801063249986\n",
      "28.098224618590166\n",
      "28.090389105124224\n",
      "28.07251338687215\n",
      "28.062090849456855\n",
      "28.049380581842144\n",
      "28.043951684980392\n",
      "28.032220806524286\n",
      "28.01968355769349\n",
      "28.01058809232851\n",
      "27.999793907488126\n",
      "27.990430813469338\n",
      "27.981407211679404\n",
      "27.971526142532436\n",
      "27.96349239554351\n",
      "27.95630210391045\n",
      "27.94961440139918\n",
      "27.937043059460553\n",
      "27.92896468881828\n",
      "27.921434189597143\n",
      "27.91051774109273\n",
      "27.90548871708188\n",
      "27.894835341755837\n",
      "27.883354248270788\n",
      "27.871667133112645\n",
      "27.861961390367004\n",
      "27.852518138355514\n",
      "27.835229690765672\n",
      "27.820377721205535\n",
      "27.81424386353166\n",
      "27.802842946618718\n",
      "27.7906846107794\n",
      "27.77827650265192\n",
      "27.76760440781149\n",
      "27.756281550065772\n",
      "27.745134003694695\n",
      "27.73561344651427\n",
      "27.723489463935397\n",
      "27.71640385229504\n",
      "27.70158453721052\n",
      "27.68931302970253\n",
      "27.68560214052754\n",
      "27.680816807366025\n",
      "27.67490809723446\n",
      "27.66583104129893\n",
      "27.661550187513804\n",
      "27.64832870636833\n",
      "27.640262603392983\n",
      "27.630252699261543\n",
      "27.619425867394813\n",
      "27.612968119004886\n",
      "27.606852571801177\n",
      "27.596254124574624\n",
      "27.58669210316131\n",
      "27.580827201352776\n",
      "27.564824259546214\n",
      "27.562388117138955\n",
      "27.559081710686893\n",
      "27.552099714001585\n",
      "27.541560286697948\n",
      "27.536892673280725\n",
      "27.528237840333784\n",
      "27.518654763464262\n",
      "27.512156693737143\n",
      "27.504733391049406\n",
      "27.497848159803283\n",
      "27.48674645163431\n",
      "27.478422361216534\n",
      "27.46973524345915\n",
      "27.456627509207635\n",
      "27.44786808938522\n",
      "27.439856521722852\n",
      "27.4326501662212\n",
      "27.427112094159835\n",
      "27.41637581250323\n",
      "27.411512278629683\n",
      "27.402613522613322\n",
      "27.392686435476648\n",
      "27.38725003662403\n",
      "27.380777316915317\n",
      "27.37224818117248\n",
      "27.365374218674766\n",
      "27.358302834143785\n",
      "27.349159691174634\n",
      "27.34392546685681\n",
      "27.33463998342965\n",
      "27.327349281574453\n",
      "27.31816064771063\n",
      "27.31457851469581\n",
      "27.31017103058579\n",
      "27.30492479655419\n",
      "27.297535271633286\n",
      "27.284275688041927\n",
      "27.278250137370893\n",
      "27.266956679614474\n",
      "27.25710419736342\n",
      "27.252238786477083\n",
      "27.247119460185043\n",
      "27.24011565317208\n",
      "27.238344743879242\n",
      "27.23003123691132\n",
      "27.223141830746332\n",
      "27.211967243382023\n",
      "27.206895320691913\n",
      "27.20071017141817\n",
      "27.196714800884088\n",
      "27.188576956955554\n",
      "27.18584794028373\n",
      "27.178830489342577\n",
      "27.175410759471767\n",
      "27.170176832542346\n",
      "27.166824713860564\n",
      "27.163277690148742\n",
      "27.156921081583185\n",
      "27.154195912199178\n",
      "27.151054298676012\n",
      "27.148209470619314\n",
      "27.13936172833236\n",
      "27.131592282004064\n",
      "27.124044140059127\n",
      "27.11628166611003\n",
      "27.110982064617627\n",
      "27.11007869044082\n",
      "27.107804364846007\n",
      "27.104332734970342\n",
      "27.099086644763137\n",
      "27.094741555102154\n",
      "27.088045503840835\n",
      "27.08458177848784\n",
      "27.081695901822552\n",
      "27.077077967944078\n",
      "27.072587805303403\n",
      "27.069258462341107\n",
      "27.060896018328872\n",
      "27.05978863061051\n",
      "27.05312030185376\n",
      "27.04875778085451\n",
      "27.04436701155746\n",
      "27.0403380372539\n",
      "27.03824414257764\n",
      "27.03319015029192\n",
      "27.03106106150161\n",
      "27.029210995332576\n",
      "27.022051271268055\n",
      "27.01912535529113\n",
      "27.011711993606806\n",
      "27.00928125352254\n",
      "27.004073518761462\n",
      "26.998230437356053\n",
      "26.995008008049222\n",
      "26.990029085152816\n",
      "26.988903911068707\n",
      "26.9843007332188\n",
      "26.9814263857754\n",
      "26.976424032542493\n",
      "26.967002447368507\n",
      "26.961667299571992\n",
      "26.957490383465114\n",
      "26.954236261118943\n",
      "26.95231721454409\n",
      "26.94825731991748\n",
      "26.94003754831583\n",
      "26.93561146685486\n",
      "26.929335027282086\n",
      "26.926971442686135\n",
      "26.919127145762875\n",
      "26.915241604870626\n",
      "26.909648989160356\n",
      "26.904894773326703\n",
      "26.896566148482798\n",
      "26.89209929027028\n",
      "26.887284384580678\n",
      "26.88296642675441\n",
      "26.875622353206445\n",
      "26.872873992378842\n",
      "26.86820161420502\n",
      "26.86559264035508\n",
      "26.864915291133133\n",
      "26.861067767509194\n",
      "26.857482400960443\n",
      "26.852693549972035\n",
      "26.847528419483115\n",
      "26.843903143840738\n",
      "26.83800340541016\n",
      "26.833807138809206\n",
      "26.828375473563934\n",
      "26.827990923078314\n",
      "26.820573256837037\n",
      "26.818013465975042\n",
      "26.819369862145262\n",
      "26.815479376973784\n",
      "26.813879746871972\n",
      "26.806687381595303\n",
      "26.803218055009836\n",
      "26.79857873127565\n",
      "26.792580357292255\n",
      "26.788848524307237\n",
      "26.785023734130018\n",
      "26.78291964113423\n",
      "26.77992684942556\n",
      "26.77727491091926\n",
      "26.77450187536657\n",
      "26.772852263312203\n",
      "26.769693214512767\n",
      "26.76672709663004\n",
      "26.764915708618304\n",
      "26.759560718709256\n",
      "26.75240961168179\n",
      "26.748364511691676\n",
      "26.744264439945887\n",
      "26.73973819750762\n",
      "26.73645605693719\n",
      "26.734522967022492\n",
      "26.732837236371214\n",
      "26.727976580097586\n",
      "26.725113369333283\n",
      "26.724773663035528\n",
      "26.72241794372079\n",
      "26.718551144805588\n",
      "26.710081300855876\n",
      "26.70910610407552\n",
      "26.705630434275506\n",
      "26.701234014866852\n",
      "26.699653851400043\n",
      "26.69702059258684\n",
      "26.693745875666554\n",
      "26.69016171387083\n",
      "26.68809551116312\n",
      "26.68365385836467\n",
      "26.68021131180369\n",
      "26.68004024892887\n",
      "26.676400238741707\n",
      "26.673071519817643\n",
      "26.668188036783434\n",
      "26.665164796892025\n",
      "26.661192404034797\n",
      "26.656117162634256\n",
      "26.653809553349184\n",
      "26.649624908665256\n",
      "26.644972199179996\n",
      "26.64307600078668\n",
      "26.642110178750166\n",
      "26.6397120605181\n",
      "26.63563149057337\n",
      "26.63528829721585\n",
      "26.631989021086536\n",
      "26.629720775052753\n",
      "26.627642738384697\n",
      "26.62320418791144\n",
      "26.618714445330397\n",
      "26.614856197103887\n",
      "26.613761902352383\n",
      "26.609598348104903\n",
      "26.609605349230424\n",
      "26.60386187211918\n",
      "26.60156642195707\n",
      "26.596360781357944\n",
      "26.590869155989882\n",
      "26.589633390332477\n",
      "26.586689435195066\n",
      "26.58427577327535\n",
      "26.580755450612244\n",
      "26.57683862649745\n",
      "26.574978092335147\n",
      "26.572875347202398\n",
      "26.571894037249535\n",
      "26.569250601084583\n",
      "26.565017929501085\n",
      "26.560735291934023\n",
      "26.559185796753816\n",
      "26.55665436299496\n",
      "26.553319911531997\n",
      "26.54892235879552\n",
      "26.54903527027542\n",
      "26.545851347826243\n",
      "26.54240290289093\n",
      "26.54064105730623\n",
      "26.535498735757297\n",
      "26.53446354112581\n",
      "26.532509258695764\n",
      "26.528661994491426\n",
      "26.526107394787722\n",
      "26.525327740881725\n",
      "26.522577879046366\n",
      "26.522935581548147\n",
      "26.521271675602065\n",
      "26.517877862535926\n",
      "26.51781634333119\n",
      "26.516932857068102\n",
      "26.511579722203237\n",
      "26.508757527255373\n",
      "26.507500160755527\n",
      "26.505612433285\n",
      "26.503103371554\n",
      "26.49957592776279\n",
      "26.497378270237526\n",
      "26.49374470086168\n",
      "26.48969045664635\n",
      "26.486889068052157\n",
      "26.482897008115863\n",
      "26.481082767146233\n",
      "26.479352482959246\n",
      "26.47870363759528\n",
      "26.475408704898786\n",
      "26.473752215737324\n",
      "26.472892058565815\n",
      "26.472309994623597\n",
      "26.470245886026753\n",
      "26.467071048509574\n",
      "26.46751517773246\n",
      "26.467412972767676\n",
      "26.46674404956854\n",
      "26.463971235321797\n",
      "26.459008355388953\n",
      "26.456574276248244\n",
      "26.454646850960437\n",
      "26.453328610603176\n",
      "26.451292689161413\n",
      "26.44812941555351\n",
      "26.44434988215485\n",
      "26.440917672162993\n",
      "26.44009505282693\n",
      "26.436916021009903\n",
      "26.434795737086475\n",
      "26.43342113738747\n",
      "26.430184684783644\n",
      "26.425811890374955\n",
      "26.42323608895714\n",
      "26.422147101013614\n",
      "26.42069531952699\n",
      "26.4205983032319\n",
      "26.418799878738216\n",
      "26.416268537507058\n",
      "26.414123900068496\n",
      "26.4121019524632\n",
      "26.411243005136768\n",
      "26.40978387029257\n",
      "26.40876101598631\n",
      "26.407082347678788\n",
      "26.405436228720305\n",
      "26.403447986587313\n",
      "26.400218892142863\n",
      "26.396489123207548\n",
      "26.394126774472177\n",
      "26.391515719101942\n",
      "26.3892375433485\n",
      "26.385992932443216\n",
      "26.385771182748844\n",
      "26.383832588480505\n",
      "26.38112607603896\n",
      "26.379436520523956\n",
      "26.374996710246783\n",
      "26.371536063771323\n",
      "26.36947589893996\n",
      "26.36587016875687\n",
      "26.362930444336733\n",
      "26.360231549394094\n",
      "26.35720478072654\n",
      "26.354089282618226\n",
      "26.353770426028902\n",
      "26.353813544879785\n",
      "26.35012477536573\n",
      "26.350502788955843\n",
      "after dev all batch: from start  24008.1, from old  3287854017.9\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "timer = ElapsedTimer()\n",
    "loss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "checkpoint_weights = np.array(checkpoint_weights) / np.sum(checkpoint_weights)\n",
    "\n",
    "dev_loss_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "val_loss_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "\n",
    "auc_array = np.zeros((n_seeds, TRAIN_ON_N_SPLITS, n_epochs))\n",
    "\n",
    "seed_oof_train_epoch_weighted_list = []\n",
    "seed_oof_train_last_list = []\n",
    "\n",
    "oof_train = np.zeros((n_seeds, n_epochs, len(x_train_indexed)))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1999)\n",
    "\n",
    "for i_seed in range(n_seeds):\n",
    "    print(f'start seed {i_seed}')\n",
    "    timer.stamp('seed start')\n",
    "    fold_dev_loss_list = []\n",
    "    fold_val_loss_list = []\n",
    "    oof_train_epoch_weighted = np.zeros(len(x_train_indexed))\n",
    "    oof_train_last = np.zeros(len(x_train_indexed))\n",
    "    \n",
    "    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_train_indexed)):\n",
    "        \n",
    "        if i_fold > TRAIN_ON_N_SPLITS:\n",
    "            continue\n",
    "        timer.stamp('epoch start')\n",
    "            \n",
    "        print(f'start fold {i_fold}')\n",
    "  \n",
    "        # Load pre-trained model (weights)\n",
    "        model = NeuralNet(y_aux_train.shape[-1], sentence_feature_mat.shape[-1]).to(device)\n",
    "        print('model', torch.cuda.memory_allocated())\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        dev_sentence_feature_mat = scaler.fit_transform(sentence_feature_mat[dev_index])\n",
    "        val_sentence_feature_mat = scaler.transform(sentence_feature_mat[val_index])\n",
    "        \n",
    "        joblib.dump(scaler, os.path.join(RESULT_PATH, f'scaler-seed{i_seed}-fold{i_fold}.joblib'))\n",
    "        \n",
    "        dev_ds = ReviewsDataset(\n",
    "            x_train_indexed[dev_index].reset_index(drop=True), \n",
    "            dev_sentence_feature_mat, \n",
    "            stacked_target[dev_index]\n",
    "        )\n",
    "        dev_loader = DataLoader(\n",
    "            dev_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        )\n",
    "        \n",
    "        valid_ds = ReviewsDataset(\n",
    "            x_train_indexed[val_index].reset_index(drop=True), \n",
    "            val_sentence_feature_mat, \n",
    "            stacked_target[val_index]\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            dev_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        )\n",
    "        \n",
    "        all_test_preds = []\n",
    "        dev_loss_list = []\n",
    "        val_loss_list = []\n",
    "        weighted_val_pred = np.zeros(val_index.shape[0])\n",
    "\n",
    "        for i_epoch in range(n_epochs):\n",
    "            timer.stamp(f'i_epoch {i_epoch} start')\n",
    "            \n",
    "            print(f'epoch_start: {i_epoch}', torch.cuda.memory_allocated())\n",
    "            start_time = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            dev_avg_loss = []\n",
    "            for i_batch, batch in enumerate(progress_bar(dev_loader)):\n",
    "                if i_epoch == 0:\n",
    "                    adjust_lr(\n",
    "                        optimizer, i_batch, min_lr=1e-6, max_lr=1e-5,\n",
    "                        n_batch_all=len(dev_loader), warm_up_batch_ratio=0.1\n",
    "                    ) \n",
    "                                \n",
    "                y_pred = model(\n",
    "                    batch['ids'].to(device), \n",
    "                    batch['attention'].to(device), \n",
    "                    batch['sent_features'].to(device)\n",
    "                )\n",
    "                \n",
    "                loss_fn = nn.BCEWithLogitsLoss(\n",
    "                    batch['sample_weight'][:, None].to(device), reduction='sum'\n",
    "                )\n",
    "                loss = loss_fn(y_pred, batch['labels'].to(device))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                dev_avg_loss += [loss.item()]\n",
    "                if i_batch % 100 == 0:\n",
    "                    print(np.hstack(dev_avg_loss).mean())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            timer.stamp(f'after dev all batch')\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            torch.save(model.state_dict(), os.path.join(RESULT_PATH, \n",
    "                f'seed{i_seed}-fold{i_fold}-epoch{i_epoch}.torchModelState'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
